#!/usr/bin/env python3
"""
Simple integration test demonstrating the complete persistence workflow.
Focuses on the business logic without complex async mocking.
"""
from unittest.mock import Mock

from app.schemas.jobspy import ScrapedJob

# Mock configuration during import and then restore to avoid side effects
import importlib, sys

_original_config = importlib.import_module("app.core.config")
_original_db = importlib.import_module("app.services.infrastructure.database")

sys.modules["app.core.config"] = Mock()
sys.modules["app.services.infrastructure.database"] = Mock()

sys.modules["app.core.config"].get_settings = lambda: Mock(database_url="fake://connection")
sys.modules["app.services.infrastructure.database"].get_database_service = lambda: Mock()

from app.services.infrastructure.job_persistence import JobPersistenceService

# Restore modules for other tests
sys.modules["app.core.config"] = _original_config
sys.modules["app.services.infrastructure.database"] = _original_db


def test_end_to_end_workflow_simulation():
    """Simulate the complete end-to-end workflow that would happen in production."""
    print("üß™ Testing end-to-end persistence workflow simulation...")
    
    # Step 1: Mock JobSpy scraping results (what we'd get from the scraper)
    scraped_jobs_raw = [
        {
            'title': 'Senior Python Developer', 
            'company': 'Tech Innovations', 
            'location': 'Remote',
            'job_type': 'Full-time',
            'date_posted': '2025-01-24',
            'salary_min': 95000.0,  # ScrapedJob expects salary_min, not min_amount
            'salary_max': 130000.0,  # ScrapedJob expects salary_max, not max_amount
            'salary_source': 'employer',
            'interval': 'yearly',
            'description': 'Exciting Python role...',
            'job_url': 'https://indeed.com/job/abc123',
            'job_url_direct': 'https://indeed.com/apply/abc123',
            'site': 'indeed',
            'emails': ['hr@tech.com'],
            'is_remote': True
        },
        {
            'title': 'Data Engineer',
            'company': 'DataFlow Corp', 
            'location': 'San Francisco, CA',
            'job_type': 'Full-time',
            'date_posted': '2025-01-23',
            'salary_min': 110000.0,  # Fixed field name
            'salary_max': 150000.0,  # Fixed field name
            'salary_source': 'glassdoor',
            'interval': 'yearly',  
            'description': 'Build data pipelines...',
            'job_url': 'https://linkedin.com/jobs/456789',
            'site': 'linkedin',
            'is_remote': False
        },
        {
            'title': '', # Missing title - should cause validation error
            'company': 'Bad Data Corp',
            'job_url': 'https://example.com/bad',
            'site': 'indeed'
        }
    ]
    
    # Step 2: Convert raw data to ScrapedJob objects (what our ingestion service does)
    scraped_jobs = []
    for raw_job in scraped_jobs_raw:
        try:
            job = ScrapedJob(**raw_job)
            scraped_jobs.append(job)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to create ScrapedJob from raw data: {e}")
    
    print(f"üì• Created {len(scraped_jobs)} ScrapedJob objects from {len(scraped_jobs_raw)} raw records")
    
    # Step 3: Test the field mapping logic
    service = JobPersistenceService()
    
    mapped_jobs = []
    validation_errors = []
    
    for i, job in enumerate(scraped_jobs):
        # Validate required fields (what persist_jobs does)
        if not job.job_url:
            validation_errors.append(f"Record {i}: missing job_url")
            continue
        if not job.title:
            validation_errors.append(f"Record {i}: missing title") 
            continue
            
        # Map to database format (what _map_job_to_db does)
        mapped = service._map_job_to_db(job, "indeed")
        mapped_jobs.append(mapped)
    
    print(f"‚úÖ Mapped {len(mapped_jobs)} valid jobs, {len(validation_errors)} validation errors")
    
    # Step 4: Verify the mapping results
    assert len(mapped_jobs) == 2, "Should have 2 valid mapped jobs"
    assert len(validation_errors) == 1, "Should have 1 validation error"
    assert "missing title" in validation_errors[0], "Error should mention missing title"
    
    # Step 5: Verify field mapping details for first job
    first_job = mapped_jobs[0]
    print(f"üîç First job mapping: {first_job}")
    assert first_job["site"] == "indeed"
    assert first_job["job_url"] == "https://indeed.com/job/abc123"
    assert first_job["title"] == "Senior Python Developer"
    assert first_job["company"] == "Tech Innovations"
    assert first_job["min_amount"] == 95000.0  # Note: should be float
    assert first_job["max_amount"] == 130000.0  # Note: should be float
    assert first_job["is_remote"] is True
    assert first_job["interval"] == "yearly"
    
    # Check source_raw contains complete original data
    source_raw = first_job["source_raw"]
    assert source_raw["title"] == "Senior Python Developer"
    assert source_raw["salary_min"] == 95000.0
    assert source_raw["job_url"] == "https://indeed.com/job/abc123"
    assert "scraped_at" in source_raw
    
    # Check nullable/future fields are properly set
    assert first_job["company_url"] is None
    assert first_job["location_country"] is None
    assert first_job["canonical_key"] is None
    
    print("‚úÖ Field mapping verification passed!")
    
    # Step 6: Verify second job (LinkedIn)
    second_job = mapped_jobs[1] 
    assert second_job["site"] == "indeed"  # Site name passed to persist function
    assert second_job["job_url"] == "https://linkedin.com/jobs/456789"
    assert second_job["title"] == "Data Engineer"
    assert second_job["is_remote"] is False
    assert second_job["min_amount"] == 110000
    
    print("‚úÖ Multi-job processing verified!")
    
    # Step 7: Simulate what the database upsert logic would do
    # In real implementation, this would be handled by PostgreSQL ON CONFLICT
    unique_keys = set()
    insert_simulation = []
    duplicate_simulation = []
    
    for job_data in mapped_jobs:
        unique_key = (job_data["site"], job_data["job_url"])
        if unique_key in unique_keys:
            duplicate_simulation.append(unique_key)
        else:
            unique_keys.add(unique_key)
            insert_simulation.append(unique_key)
    
    # Simulate persistence summary
    mock_summary = {
        "inserted": len(insert_simulation),
        "skipped_duplicates": len(duplicate_simulation),
        "errors": validation_errors
    }
    
    print(f"üìä Simulated persistence summary: {mock_summary}")
    
    assert mock_summary["inserted"] == 2
    assert mock_summary["skipped_duplicates"] == 0  
    assert len(mock_summary["errors"]) == 1
    
    print("‚úÖ Complete end-to-end workflow simulation passed!")


def test_api_response_format():
    """Test what the API response would look like with persistence data."""
    print("üß™ Testing API response format...")
    
    # Simulate successful scraping + persistence
    mock_scrape_result = {
        "status": "succeeded",
        "jobs": [
            ScrapedJob(
                title="DevOps Engineer",
                company="Cloud Systems",
                job_url="https://glassdoor.com/job/789",
                site="glassdoor"
            )
        ],
        "total_found": 1,
        "message": "Successfully scraped 1 job"
    }
    
    mock_persistence_summary = {
        "inserted": 1,
        "skipped_duplicates": 0,
        "errors": []
    }
    
    # Simulate the API response format from our updated jobspy.py
    api_response = {
        "jobs": [job.model_dump() for job in mock_scrape_result["jobs"]],
        "total_found": mock_scrape_result["total_found"],
        "search_metadata": {"site": "glassdoor"},
        "execution_mode": "sync",
        "persistence_summary": mock_persistence_summary
    }
    
    print(f"üì° API Response structure: {list(api_response.keys())}")
    
    # Verify the response has all expected components
    assert "jobs" in api_response
    assert "total_found" in api_response
    assert "persistence_summary" in api_response
    assert api_response["persistence_summary"]["inserted"] == 1
    assert api_response["execution_mode"] == "sync"
    
    # Verify jobs data format
    assert len(api_response["jobs"]) == 1
    assert api_response["jobs"][0]["title"] == "DevOps Engineer"
    assert api_response["jobs"][0]["job_url"] == "https://glassdoor.com/job/789"
    
    print("‚úÖ API response format verification passed!")


if __name__ == "__main__":
    print("üß™ Running end-to-end persistence workflow tests...\n")
    
    test_end_to_end_workflow_simulation()
    print()
    test_api_response_format()
    
    print("\nüéâ All workflow tests passed!")
    print("\nüìã What was tested:")
    print("   ‚úÖ Raw JobSpy data ‚Üí ScrapedJob object conversion")
    print("   ‚úÖ ScrapedJob ‚Üí database field mapping")
    print("   ‚úÖ Validation of required fields (job_url, title)")
    print("   ‚úÖ Source data preservation in jsonb field")
    print("   ‚úÖ Proper handling of nullable/placeholder fields")
    print("   ‚úÖ Multi-job batch processing")
    print("   ‚úÖ Persistence summary generation")
    print("   ‚úÖ API response format with persistence data")